\section{I did a big oof and i wanna talk about it}
In the next section we will cover our experiments with PassGAN and the Libero dataset.
before that however we need to address some fundamental issues with the dataset: First, our copy of the Libero database onnly contained plain texts passowrds, and secondly we were unable to verify the exact provenance of this data.
A direct link to the data can be found on the \texttt{databases.today} website\footnote{\url{https://cdn.databases.today/Libero.it\%20900k.zip}}

As regards the first issue, as mentioned in section \ref{sec:related_work} the Libero set is a JSON formatted file containing various pieces of information for each of user (above 600,000 user); common fields for each user include email addrress, user-name and internal User ID, but some users in the document also have a real name attached. As for the passowrds, each user object contains both a plaintext password and an MD5 hash, but upon closer inspection we find that there is a problem: the MD5 hash is the same for each user, an MD5 hash of the word \enquote{boomerang}.

We have no idea as to why the uploader of the original cracker would choose to do something like that, and no clear evidence towards any particular reason; The following is purely our speculation, but we can imagine for example, that the attacker might decide to throw away the original hash and only keep the paintext of the password in order to keep the information for each user organized and more easily exploitable: since he exfiltrated not just password but also personal information like names and addresses for some of the users, one might imagine that it would be convenient to organize all the information for each user in a single JSON object.

This alteration to what we might normally expected to find - a file containing both plain texts and hashes - made us question the autenticity of the data: while we were unable to find definitive proof, the number of acounts present in our file is roughtly consisstent with the number reported in \cite{libero_leak}; further more, the hash of the archive matches that of a VirusTotal report, that shows the archive first appeared on VirusTotal in December 2016, a couple of months after news broke about the Libero Mail security breach \cite{virus_total}.
Italian news artices reporting on the incident came out around September  7th 2016, when Libero Mail sent an email to their customers informing them of the breach \cite{libero-news-wired,libero-news-tomhw,libero-news-fanpage}.
Looking at the database file itself, its last modified timestamp is dated to the 25th of September, placing its origin somewhere closer to the date of the incident: Unfortunately neither the news articles nor Libero Mail themselves give an indication of when exactly the incident occourred, so its hard to judge the exact timeline of events; Upon reading the email that Libero Mail sent to customers on September 7th (available in \cite{libero-news-fanpage} in Italian), the language use suggests they may have been aware of the breach for a considerable amount of time before breaking the news to customers.

The elements above leads to beleive that the data we have is geninuinely from the Libero leak, though ultimately its very hard to know for certain: these acounts and passwords have been included in a variety of other pastes ever since the incident, and if the original leak came from the Tor network like the Virus Total report tags seem to suggest, it might be inpossible to track down the original archive and/or creator.

The lack of hashed passwords in our copy of the Libero leak did not interfere with training of the PassGAN system, as the input data consisted of paintext passowrds, but it did pose a challenge for testing: in order to test PassGAN we needed to crack the Libero leak passwords using HashCat, and thus we needed to turn the paintext passwords back into hashed passwords.
In ordered to do that we hashed the paintext passwords with MD5 and used the resulting file for testing: our choiche of MD5 was somewhat arbitrary, as there is no clear indication of how the passwords might have been stored in the Libero Mail database: we chose MD5 because the filename of the database file hinted that that might have been the hashing alorithm used originally, and also because the weakness of MD5 would make the cracking process much faster compared to using SHA-1 or SHA-256; the JSON file contained no explicit idication of salt usage, so we decided to not salt the password either. However for all the reasons idiscussed above, its possible that Libero Mail used a more secure storage method with different hashing algorithms and salts, we simply do not have enough information to express a judgement either way.

We are aware that this lack of information and authenticity hurts the applicability of this paper, because of these compromises we cannot claim that our results are 100\% applicable in the real world. Out argument would have been more robust if we had a dataset that more closely matched the original way in which passwords were stored, but we argue that this paper is not focused on the real-world applicability of Machine Learning systems like PassGAN for password cracking. 
Rather, we view the Libero leak as an example, a source of data to evaluate PassGAN when used on non-english passwords and expeiment with the impact on Natural Language on password cracking.
\newpage
In order to prepare the data from the Libero leak for both training of PassGAN and subsequent testing we used a combination of UNIX shell commands, displayed below:

\begin{lstlisting}[language=bash,numbers=left,stepnumber=1,breaklines=true,postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}]
#Extract passwords from the databse file
cat libero.it-lebero-poinx21.pxusers-plaintext-md5-2016-09-json-900k-users-extremely-private.txt|grep clearPassword |cut -d ":" -f 2 | awk '{gsub ("\"","");gsub(",","");print \$1}' > passwords.txt

#Split the entire password dataset into 80\%/20\%
head -n 534171 passwords.txt|shuf > train.txt
tail -n 133542 passwords.txt|shuf > test.txt

#Extract passwords that are 10 characters or less (needed for testing)
grep -x '.\{1,10\}' test.txt > test_10c.txt

#Generate MD5 hashes of the testing set for Hascat
for i in $(cat test_10c.txt); do echo -n "$i"| md5sum | tr -d " -" >> attempt2/test_10c.hash; done

#Generalized parameters for Hashcat
hashcat -a 0 -m 0 --potfile-disable test_10c.hash $WORDLIST -r $RULESET -o out.txt
\end{lstlisting}

