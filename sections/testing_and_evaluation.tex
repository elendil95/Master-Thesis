\section{Testing and evaluation}
In this next section, we are going to evaluate the performance of the PassGAN system, as well as give a brief overview of our methodology and the experimental setup.

\subsection{Experimental setup}
Both training og the GAN and testing were carried out on a Linux machine running Debian stretch, equipped with a GTX 1070 graphics card, an i5-2500 CPU and 8GB of system memory. 
With this card, training of PAssGAN took roughtly eleven hours.
PassGAn was run on Python 2.7, running Tensorflow 1.12.0.\\
For testing we used the latest stable verion of Hashcat as of this writing (version 5.1.0) and we observed a peak speed between 850 and 860MH/s (for reference, 1MH/s is equivalent to 1 million candidate passwords hashed and compared per second) %Reference   

\subsection{Testing the PassGAN system}
In order to train the PassGAN system, we extracted the plain text passowrds from the original file (which was formatted in JSON as it contained personal information beyon just the emails and passowrds of the users), and split it up in two sets: a training set containing 80\% of the passowrds (534,171 entries) and a testing set containing 20\% of the passowrds (133,542 entries).
We used the modedl trained on the the biggger set to generate 1 million samples from PassGAN, that then served as the basis from our testing.

Below is an example of samples that were produced by PassGAN at the very end of training: %Maybe i should take a sample from the last checkpoint instead?
\begin{verbatim}
22052906    mariamuna   �iaup63     poldetta53
001178      04051987    topolapa    aenara
rotsanera   princone    fadyreda21  giugki
dimonepa72  12orlomon   deb57828    belnabola
marcanalla  ACADCNA�N   leegenniki  vicaletto
\end{verbatim}

What we found particularly interesting was that while the samples showed the typical patterns exhibited by user-generated Passwords (numbers appended or pre-pended to words, leet speak etc..), a majority of them also seemed to mimick italian words and phrases: most of these words were meaningless, but nonetheless we spwculated that the GAn might be trying to "learn" Italian as a side-effect of generating samples close to the inpout data distribution.

During our testing we tried to follow the same sort of methodology that Hitaj et al.\cite{PassGAN} used in their paper: as such, in our testing we used a subset of both our training  and our testing set, composed of passowrds with a lenght of 10 characters or less. This was done in order to focus the scope of PassGAN, as during trainign we set the sequance lenght to 10 %Bullshit you just wanna give yourself a leg up.

In order to test the Performance of PassGAN, we compared it with other wordlists and rules commonly used in rule-based password crackers: Specifically we chose \emph{RockYou} as our wordlist, longside \emph{best64} and \emph{generated2} as our rules; All of them were chosen because they were also used by Hitaj et al.\cite{PassGAN}.

The RockYou wordlist containst more than 14 million strings, all of which come from various data breaches that have been compiled into one resource;it countains password of wildly varying lenght and complexity, and as such should provide a good performance baseline for rule-based crackers. The best64 rules file contains around 70 mangling rules, while the genrated2 rules file contains more than 65.000. 
Finally in order to test the output of PassGAN we used a wordlist containing 1 million samples generated by the model we trained earlier.

The tables below will illustrate the results of our tests, performed on both the Training set and the Testing set using the wordlists and rules described above. To clarify, we decided early on to use mangling rules also when testing password samples from PasGAn, sincec they greatly increased the number of passwords found.
